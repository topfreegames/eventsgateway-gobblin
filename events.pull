job.name=events
job.group=events
job.description=events
job.lock.enabled=true
job.schedule=0 0/10 0/1 1/1 * ? *

kafka.brokers=${env:KAFKA_BROKERS}
kafka.workunit.packer.type=BI_LEVEL

source.class=gobblin.source.extractor.extract.kafka.KafkaSimpleSource
extract.namespace=gobblin.extract.kafka
extract.limit.enabled=true
extract.limit.type=time
extract.limit.timeLimit=120
extract.limit.timeLimitTimeunit=minutes

converter.classes=com.tfgco.eventsgateway.gobblin.GobblinBytesToAvroConverter

writer.partitioner.class=com.tfgco.eventsgateway.gobblin.GobblinDailyPartitioner
writer.partition.pattern=YYYY/MM/dd
writer.partition.columns=timestamp
writer.builder.class=gobblin.writer.AvroDataWriterBuilder
writer.file.path.type=tablename
writer.partition.timezone=UTC
writer.destination.type=HDFS
writer.output.format=AVRO

writer.codec.type=SNAPPY
writer.deflate.level=6
writer.include.partition.in.file.names=false

mr.job.max.mappers=5

topic.whitelist=sv-uploads-.*
data.publisher.type=gobblin.publisher.TimePartitionedDataPublisher

metrics.reporting.file.enabled=true
metrics.log.dir=${env:GOBBLIN_WORK_DIR}/metrics
metrics.reporting.file.suffix=txt

email.alert.enabled=true
email.host=${env:EMAIL_HOST}
email.smtp.port=25
email.user=${env:EMAIL_USER}
email.password=${env:EMAIL_PASSWORD}
email.from=${env.EMAIL_FROM}
email.tos=${env.EMAIL_TOS}
